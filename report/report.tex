\documentclass[11pt,conference,compsocconf]{IEEEtran}
\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{color}
\usepackage{amsmath,amsfonts,amssymb}%Includes vector notation, etc.
\newcommand{\R}{\mathbb{R}}%Set of real numbers
\usepackage{listings}  %For code strings
\lstset{language=Python}   
\usepackage[font=small]{caption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{multirow} %These 4 are for tables
\usepackage{multicol}
\usepackage{tabularx}
\usepackage{tablefootnote}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{url}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\begin{document}
\font\myfont=cmr12 at 20pt
\title{\myfont EPFL Deep Learning course, Project 1 \\ Finger Movements Predictor}

\author{Diego Fiori, Leonardo Petrini, Stefano Savar\`{e}}

\maketitle

\begin{abstract}

    The purpose of this miniproject is to develop a deep learning framework from scratch using only \texttt{PyTorch} tensors and the \texttt{math} library. The framework provides all the tools to build a simple network composed of linear layers, ReLU and Tanh activation functions and MSE loss. Each with the methods to compute forward and backward passes. The network is trained with stochastic gradient descent.
    
    In order to test the framework, we build a simple dataset and train an instance of the network on it.

\end{abstract}

\section{Introduction}

The report is structured in the following way:

\begin{itemize}
    \item Sections II and III describe the general structure of the framework
    \item Sections IV, V and VI describe how the network is trained to classify points on a synthetic dataset.
\end{itemize}

\section{Framework structure 
\footnote{Lowercase letters indicate vectors, capital letters are used for matrices. The loss function $\mathcal{L}$ is a scalar, instead. For each component of the networks, $x$ indicate its input, $y$ its output and $\hat{y}$ is used for the target label of the final layer.}
}

\subsection{\texttt{Module} class}

All the classes\footnote{Except the optimizer.} inherit from the \texttt{Module} class, which then establishes the general structure to be used.

\subsection{Linear layer}

When instantiated, a linear layer takes number of \texttt{inputs} and number of units as arguments. Weights and bias parameters are initialized randomly accordingly to a Gaussian $\mathcal{N}(0, 0.1)$ and their gradients are set to zero.

When called, the forward pass computes

\begin{equation*}
    y = W x + b.
\end{equation*}

The backward pass both update the gradient of the parameters 

\begin{equation*}
    \frac{d\mathcal{L}}{dW} = \frac{dy}{dW} \frac{d\mathcal{L}}{dy} = x^{T}\:\frac{d\mathcal{L}}{dy} 
\end{equation*}

and 

\begin{equation*}
    \frac{d\mathcal{L}}{db} = \frac{dy}{db} \frac{d\mathcal{L}}{dy} = \mathbbm{1} \frac{d\mathcal{L}}{dy} 
\end{equation*}

and backpropagate the derivatives

\begin{equation*}
    \frac{d\mathcal{L}}{dx} = \frac{d\mathcal{L}}{dy}\: W^{T}.
\end{equation*}


\subsection{Activation functions}

We implemented ReLU and Tanh activation functions. Each is a class with both forward and backward methods. For the first activation we have, respectively,

\begin{equation*}
    y = \max\:(0, x),
\end{equation*}

and 

\begin{equation*}
    \frac{d\mathcal{L}}{dx} = \frac{d\mathcal{L}}{dy} \: \mathbbm{1}_{[x > 0]}.
\end{equation*}

Similarly, for Tanh,

\begin{equation*}
    y = \tanh{x},
\end{equation*}

\begin{equation*}
    \frac{d\mathcal{L}}{dx} = \frac{d\mathcal{L}}{dy} \cdot (1 - \tanh^2{x}).
\end{equation*}

\subsection{Loss function}

As a loss function we used the mean square error. Namely

\begin{equation*}
    \mathcal{L} = \frac{1}{N} \: \sum_{\texttt{dataset}} (y - \hat{y})^2
\end{equation*}

whose derivative for the backward pass is  

\begin{equation*}
    \frac{d\mathcal{L}}{dy} = \frac{2}{N}\: (y - \hat{y}).
\end{equation*}
As a sidenote, during training the factor $1/N$ is neglected. This choice is balanced while determining the proper learning rate.

\subsection{Sequential class}

Finally, the actual network is an instance of the \texttt{Sequential} class which takes a list of linear layers and activations and calls the forward and backward passes of each layer in the required order.

\subsection{Optimizer}

The last module needed to perform the training is the optimizer. It takes as arguments the parameters of the networks and the learning rate and performs an optimization step, namely it update all the network's parameters.

As an additional parameter one can provide a decaying rate for the learning rate. Then when \texttt{optimizer.\_lr\_decay()} is called it computes

\begin{equation*}
    \texttt{learning\_rate *= decay\_rate}.
\end{equation*}
This is typically done each epoch.


\section{Training and testing}

Training can be performed for several epochs. Stochastic gradient descent with batches is used. Each epoch a new permutation of the dataset is computed and used to obtain gradients and update weights. 

The parameters to be set are then:
\begin{itemize}
    \item Total \texttt{number of epochs},
    \item \texttt{Mini-batch size},
    \item \texttt{Learning rate} and its \texttt{decay rate}.
\end{itemize}




\section{Dataset}

Both train and test datasets are made taking $1'000$ random points in the region $[0,1]\times[0,1]$ and then labels in $\{0,1\}$ are assigned to each point depending on their position with respect to the circle centered in $(0.5, 0.5)$ with radius $\frac{1}{\sqrt{2\pi}}$, i.e. 0 if outside the disk, 1 inside. The resulting dataset is shown in Figure \ref{fig:dataset}.

We have a classification problem with two classes which are perfectly balanced considering the area of the disk is $1/2$ the area of the square of edge 1.

\begin{figure}
    \centering
    \includegraphics[scale=.5]{dataset.png}
    \caption{Visualization of the dataset.}
    \label{fig:dataset}
\end{figure}

\section{The network instance}

We train a network which takes two inputs -- the position in $[0,1]\times[0,1]$ -- and returns two outputs. Considering the last activation function is the Tanh, target outputs of $(-1, 1)$ if inside the disk, and $(1, -1)$ if outside are used. The hidden layers are 3, each of 25 units fully connected with ReLU activation functions. 

The sequential structure of the net is then:

\begin{flushright}
\texttt{2 inputs $\rightarrow$ 25 units FC $\rightarrow$ ReLU $\rightarrow$ \\   25 units FC $\rightarrow$ ReLU $\rightarrow$ \\ 25 units FC $\rightarrow$ ReLU  $\rightarrow$ \\
2 units FC $\rightarrow$ Tanh $\rightarrow$ \\
$\rightarrow$ 2 outputs}.
\end{flushright}

The loss function is the MSE and the network is trained with SGD -- batches of size 10 are used.

The network is trained for 500 epochs, the initial learning rate is $2\times10^{-2}$ and it decays of $2\%$ per epoch. 



\section{Results}

The networks is able to correctly classify all the points in the training dataset ($(99.9 \pm 0.1) \%$ accuracy) while it reaches a generalization accuracy of $ (98.8 \pm 0.3) \%$ on the test. Accuracy averages and standard deviations are obtained over 50 different realizations of the same setting.


Training loss and the test accuracy as a function of the epochs are shown in Figures \ref{fig:loss} and \ref{fig:accuracy} respectively.

\begin{figure}
    \centering
    \includegraphics[scale=.6]{loss_loglog_2.eps}
    \caption{Training loss. Both axis are in log-scale.}
    \label{fig:loss}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=.6]{accuracy_2}
    \caption{Test accuracy. On the x-axis the epoch number, on the y-axis the percentage of correctly classified samples.}
    \label{fig:accuracy}
\end{figure}

Figure \ref{fig:dataset_epochs} we show how the classification gets better during training. In a few epochs (10) the network is able to correctly classify $90\%$ of the points, the shape of the disk is already visible. All the remaining epochs are needed to better define the circle perimeter.


%\newpage

\begin{figure}
    \centering
    \includegraphics[scale=.8]{dataset_training_1_15_15_50_100_500.png}
    \caption{Classification evolution during training. The labels color reflects the output of the network. These scatter plots correspond to epoch number 1, 10, 25, 50, 100, 200.}
    \label{fig:dataset_epochs}
\end{figure}


\bibliographystyle{IEEEtran}
\bibliography{literature}


\end{document}

