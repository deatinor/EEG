{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the nn class **Sequential** to model a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the single target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.039674Z",
     "start_time": "2018-04-29T15:45:46.586434Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.063733Z",
     "start_time": "2018-04-29T15:45:48.042443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import load_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.086364Z",
     "start_time": "2018-04-29T15:45:48.065870Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.583592Z",
     "start_time": "2018-04-29T15:45:48.088414Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset,target=load_script.load_dataset(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.754805Z",
     "start_time": "2018-04-29T15:45:48.585273Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset,test_target=load_script.load_dataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.768848Z",
     "start_time": "2018-04-29T15:45:48.756513Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target=target.long()\n",
    "test_target=test_target.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Added modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.783664Z",
     "start_time": "2018-04-29T15:45:48.770418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.798360Z",
     "start_time": "2018-04-29T15:45:48.785171Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tensor4D(nn.Module):\n",
    "    def forward(self,input):\n",
    "        return input.view(input.shape[0],1,*input.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.812756Z",
     "start_time": "2018-04-29T15:45:48.799877Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tensor3D(nn.Module):\n",
    "    def forward(self,input):\n",
    "        return input.view(input.shape[0],input.shape[1],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:48.828631Z",
     "start_time": "2018-04-29T15:45:48.814246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 7.3101e+01  7.3109e+01  7.2001e+01  ...   6.4324e+01  6.5037e+01  6.5048e+01\n",
       " 5.0706e+00  5.9728e+00  4.1829e+00  ...  -3.1212e+00 -2.7500e+00 -2.3117e+00\n",
       " 5.3022e+01  5.3706e+01  5.2293e+01  ...   4.5319e+01  4.5553e+01  4.6292e+01\n",
       "                ...                   ⋱                   ...                \n",
       " 1.3597e+01  1.3734e+01  1.3298e+01  ...   1.0306e+01  1.0556e+01  1.0504e+01\n",
       " 2.3973e+01  2.3983e+01  2.2980e+01  ...   2.2531e+01  2.2226e+01  2.2265e+01\n",
       " 9.5851e+00  9.5892e+00  8.4953e+00  ...   8.9079e+00  9.0392e+00  8.8203e+00\n",
       "[torch.FloatTensor of size 28x50]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([316, 28, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:49.133752Z",
     "start_time": "2018-04-29T15:45:49.109222Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean=train_dataset.mean(0).view(1,28,-1)\n",
    "std=train_dataset.std(0).view(1,28,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:45:49.557152Z",
     "start_time": "2018-04-29T15:45:49.525799Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset=(train_dataset-mean)/std\n",
    "test_dataset=(test_dataset-mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:55:20.996042Z",
     "start_time": "2018-04-29T15:55:20.964306Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyConv1D:\n",
    "    def __init__(self,input_channels,output_channels,kernel,dropout_rate=0.8,batch_norm=True):\n",
    "        self.conv=nn.Conv1d(input_channels,output_channels,kernel)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.batch_norm=nn.BatchNorm1d(output_channels)\n",
    "        self.dropout=nn.Dropout(dropout_rate)\n",
    "        if batch_norm:\n",
    "            self._layers=[self.conv,self.relu,self.batch_norm,self.dropout]\n",
    "        else:\n",
    "            self._layers=[self.conv,self.relu,self.batch_norm,self.dropout]\n",
    "    \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self._layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThreeLayersParams(nn.Module):\n",
    "    \n",
    "    num_my_conv_layers=3\n",
    "    num_linear_layers=2\n",
    "    \n",
    "    def __init__(self,params):\n",
    "        super(ThreeLayersParams,self).__init__()\n",
    "        \n",
    "        self.params=params\n",
    "        \n",
    "        layers=[]\n",
    "        for i in range(self.num_my_conv_layers): \n",
    "            layers+=MyConv1D(*self.params[i]).layers\n",
    "        \n",
    "        layers.append(Flatten())\n",
    "        a=[*self.params[self.num_my_conv_layers]]\n",
    "        layers.append(nn.Linear(*self.params[self.num_my_conv_layers]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(*self.params[self.num_my_conv_layers+1]))\n",
    "        \n",
    "        self.sequential=nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.sequential(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerParams:\n",
    "    def __init__(self,*args):\n",
    "        self._params=[*args]\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self._params)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        return self._params[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayersParams:\n",
    "    \n",
    "    def __init__(self,*args):\n",
    "        self.params=[*args]\n",
    "        \n",
    "    def __getitem__(self,key):\n",
    "        return self.params[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    ''' Class that defines the parameters of a network evaluation.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    _input_shape=(28,50)\n",
    "    \n",
    "    def __init__(self,network_type,\n",
    "                 optimizer_type,\n",
    "                 criterion_type,\n",
    "                 mini_batch_size=79,\n",
    "                 max_epoch=1000,\n",
    "                 dropout_rate=0.8,\n",
    "                 batch_norm=True,\n",
    "                 conv_filters=False,\n",
    "                 conv_kernels=3,\n",
    "                 linear_filters=False,\n",
    "                 learning_rate=0.001,\n",
    "                 weight_decay=0,\n",
    "                 weights_initialization=0.02,\n",
    "                 randomize_training_dataset=True,\n",
    "                 plot=True,\n",
    "                 verbose=False):\n",
    "        \n",
    "        # Set up network\n",
    "        self.network_type=network_type\n",
    "        self._layer_params=self.set_up_network_params(conv_filters,conv_kernels,dropout_rate,batch_norm,linear_filters)        \n",
    "        self._network=network_type(self._layer_params)\n",
    "\n",
    "        # Set up optimizer\n",
    "        self._optimizer=optimizer_type(self._network.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "\n",
    "        # Set up criterion\n",
    "        self._criterion=criterion_type()\n",
    "        \n",
    "        # Set up training parameters\n",
    "        self._mini_batch_size=mini_batch_size\n",
    "        self._max_epoch=max_epoch\n",
    "        self._weights_initialization=weights_initialization\n",
    "        self._randomize_training_dataset=randomize_training_dataset\n",
    "        \n",
    "        # Set up visualization parameters\n",
    "        self._plot=plot\n",
    "        self._verbose=verbose\n",
    "        \n",
    "    def set_up_network_params(self,conv_filters,conv_kernels,dropout_rate,batch_norm,linear_filters):\n",
    "        # We automatically create LayersParams based on the input given\n",
    "        self._num_my_conv_layers=self.network_type.num_my_conv_layers\n",
    "        self._num_linear_layers=self.network_type.num_linear_layers\n",
    "        \n",
    "        # Set up conv_filters\n",
    "        self._conv_filters=[self._input_shape[0]]+\\\n",
    "                    self.add_params_sequence(self._num_my_conv_layers,conv_filters,self._input_shape[0])\n",
    "        \n",
    "        # Set up conv_kernel\n",
    "        self._conv_kernels=self.add_params_sequence(self._num_my_conv_layers,conv_kernels,3)\n",
    "        \n",
    "        # Set up dropout\n",
    "        self._dropout_rate=self.add_params_sequence(self._num_my_conv_layers,dropout_rate,0.2)\n",
    "        \n",
    "        # Set up batch norm\n",
    "        self._batch_norm=self.add_params_sequence(self._num_my_conv_layers,batch_norm,True)\n",
    "        \n",
    "        \n",
    "        # Set up linar_layers\n",
    "        self._linear_layer_start_filters=int(self._conv_filters[-1]*(self._input_shape[1]-\n",
    "                                        np.sum([(x-1) for x in self._conv_kernels])))\n",
    "        print(self._conv_filters[-1])\n",
    "        self._linear_filters=[self._linear_layer_start_filters]+\\\n",
    "                        self.add_params_sequence(self._num_linear_layers,linear_filters,False)\n",
    "        \n",
    "        self._layer_params_list=[]\n",
    "        for i in range(self._num_my_conv_layers):\n",
    "            self._layer_params_list.append(LayerParams(*self._conv_filters[i:i+2],self._conv_kernels[i],self._dropout_rate[i]))\n",
    "        for i in range(self._num_linear_layers):\n",
    "            self._layer_params_list.append(LayerParams(*self._linear_filters[i:i+2]))\n",
    "        \n",
    "        _layer_params=LayerParams(*self._layer_params_list)\n",
    "        return _layer_params\n",
    "        \n",
    "    def add_params_sequence(self,num_layers,params,default_value):\n",
    "        if not params:\n",
    "            params=[default_value]*num_layers\n",
    "        try:\n",
    "            if len(params)!=num_layers:\n",
    "                params=[default_value]*num_layers\n",
    "        except:\n",
    "            params=[params]*num_layers\n",
    "            \n",
    "        return params\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def network(self):\n",
    "        return self._network\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "    @property\n",
    "    def criterion(self):\n",
    "        return self._criterion\n",
    "    @property\n",
    "    def mini_batch_size(self):\n",
    "        return self._mini_batch_size\n",
    "    @property\n",
    "    def max_epoch(self):\n",
    "        return self._max_epoch\n",
    "    @property\n",
    "    def weights_initialization(self):\n",
    "        return self._weights_initialization\n",
    "    @property\n",
    "    def randomize_training_dataset(self):\n",
    "        return self._randomize_training_dataset\n",
    "    @property\n",
    "    def plot(self):\n",
    "        return self._plot\n",
    "    @property\n",
    "    def verbose(self):\n",
    "        return self._verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:56:27.249573Z",
     "start_time": "2018-04-29T15:56:27.073138Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Train:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self,params,repetitions=5):\n",
    "        epochs=[]\n",
    "        errors_max=[]\n",
    "        errors_mean=[]\n",
    "        for i in range(repetitions):\n",
    "            for param in params.network.parameters():\n",
    "                param.data.normal_(0, params.weights_initialization)\n",
    "            print('Repetition',i)\n",
    "            epoch,error_max,error_mean=self.train_test_network(params)\n",
    "            epochs.append(epoch)\n",
    "            errors_max.append(error_max)\n",
    "            errors_mean.append(error_mean)\n",
    "        \n",
    "        return epochs,errors_max,errors_mean\n",
    "    \n",
    "    \n",
    "    def train_test_network(self,params):\n",
    "    \n",
    "        errors_train=[]\n",
    "        errors_test=[]\n",
    "        \n",
    "        network=params.network\n",
    "        \n",
    "        for epoch in tqdm(range(params.max_epoch)):\n",
    "            \n",
    "            total_loss,output_train,error_train=self.train_epoch(params)\n",
    "            output_test,error_test=self.test_epoch(params)\n",
    "        \n",
    "        \n",
    "            errors_train.append(error_train)\n",
    "            errors_test.append(error_test)\n",
    "            \n",
    "            if epoch%10==0 and params.verbose:\n",
    "                print('Epoch:',epoch,'Loss:',total_loss,'Correct:',str(error_train*100)[:5]+\"%\",\n",
    "                     'Correct test:',str(error_test*100)[:5]+\"%\")\n",
    "                \n",
    "        if params.plot:\n",
    "            \n",
    "            print('Performance:',np.mean(errors_test[300:]))\n",
    "            plt.plot(list(range(epoch+1)),errors_train,label='Errors train')\n",
    "            plt.plot(list(range(epoch+1)) ,errors_test,label='Errors test')\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "                 \n",
    "        return epoch,np.max(errors_test),np.mean(errors_test[300:])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_epoch(self,params):\n",
    "        \n",
    "        # Set training True\n",
    "        params.network.train(True)\n",
    "        \n",
    "        # Randomize training dataset\n",
    "        if params.randomize_training_dataset:\n",
    "            random_permutation=torch.randperm(train_dataset.shape[0])\n",
    "        else:\n",
    "            random_permutation=torch.arange(train_dataset.shape[0])\n",
    "\n",
    "        train_dataset_shuffled=train_dataset[random_permutation]\n",
    "        target_shuffled=target[random_permutation]\n",
    "        \n",
    "        \n",
    "        # Iterate on the dataset\n",
    "        total_loss=0\n",
    "        output_target=torch.zeros(target_shuffled.shape[0])\n",
    "        for b in range(0,train_dataset_shuffled.shape[0],params.mini_batch_size):\n",
    "\n",
    "            train_element=train_dataset_shuffled.narrow(0,b,params.mini_batch_size)\n",
    "            target_element=target_shuffled.narrow(0,b,params.mini_batch_size)\n",
    "\n",
    "            params.optimizer.zero_grad()\n",
    "\n",
    "            out=params.network(train_element)\n",
    "            output_target[b:b+params.mini_batch_size]=(out[:,1]>out[:,0]).data\n",
    "\n",
    "            loss=params.criterion(out,target_element)\n",
    "            loss.backward()\n",
    "            params.optimizer.step()\n",
    "            total_loss+=loss.data[0]\n",
    "            \n",
    "        error_train=np.sum(list(output_target.long()==target_shuffled.data))/target.shape[0]    \n",
    "        \n",
    "        return total_loss,output_target,error_train\n",
    "    \n",
    "    def test_epoch(self,params):\n",
    "        params.network.train(False)\n",
    "        output_test=torch.zeros(test_target.shape[0])\n",
    "        out=params.network.forward(test_dataset)\n",
    "        output_test=(out[:,1]>out[:,0]).data.long()\n",
    "        error_test=np.sum(list(output_test==test_target.data))/test_target.shape[0]\n",
    "        \n",
    "        return output_test,error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/400 [00:00<00:48,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "Repetition 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399/400 [00:41<00:00,  9.51it/s]"
     ]
    }
   ],
   "source": [
    "net_type=ThreeLayersParams\n",
    "optimizer_type=optim.Adam\n",
    "criterion_type=nn.CrossEntropyLoss\n",
    "params=Params(net_type,optimizer_type,criterion_type,linear_filters=[200,2],max_epoch=400)\n",
    "\n",
    "train=Train()\n",
    "train(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
